---
title: "Prediction"
output: html_notebook
---


## packages

```{r}
# devtools::install_github("systats/tidyTX")
pacman::p_load(dplyr, ggplot2, googlesheets, openxlsx, stringr, rvest, dplyr, ggplot2, keras, mlrMBO, tidyMBO, ggthemes, Smisc, randomForest, parallelMap, emoa)
#devtools::install_github("systats/tidyMBO")
set.seed(2018)
ggplot2::theme_set(ggthemes::theme_few())
```


## Prep

```{r}
# googlesheets::gs_auth(token = "shiny_app_token.rds")
# with_label <- gs_title("altright_data_final") %>%
#   gs_read()
# save(with_label, file = "data/with_label.Rdata")
load("data/with_label.Rdata")

clean_metric <- function(x){
  x %>%
    str_replace_all("Not Present", "1") %>%
    str_replace_all("Strongly Present", "5") %>%
    str_replace_all("99", "0") %>% 
    as.numeric()
}

df_metric <- with_label %>%
  dplyr::select(identity:left, anti_fem:anti_mus) %>%
  purrr::map_df(.f = ~clean_metric(.x)) %>%
  purrr::map_df(.f = ~ifelse(.x == 1, 0, 1))


df_order <- with_label %>%
  dplyr::select(user:nchar, coder, timestamp)

clean_category <- function(x){
  if(is.logical(x)) return(x)
  x %>%
    str_replace_all("99", "0") %>% 
    as.numeric()
}

df_category <- with_label %>%
  dplyr::select(lang:irony) %>%
  purrr::map_df(.f = ~clean_category(.x))
 
 
df_all <- bind_cols(
    df_order,
    df_metric,
    df_category
  ) %>%
  filter(!duplicated(text)) %>%
  mutate(
    platform = case_when(
      platform == "fb" ~ "Facebook",
      platform == "tw" ~ "Twitter",
      platform == "yt" ~ "YouTube"
    )
  ) %>%
  mutate(altright = case_when(
    identity > 0 ~ 1 ,
    race > 0 ~ 1 ,
    anti_sem > 0 ~ 1 ,
    moral > 0 ~ 1 ,
    imm == 3 ~ 1 ,
    vict == 2 ~ 1 ,
    TRUE ~ 0
  )) %>% 
  mutate(altlight = case_when(
    anti_mus > 0 ~ 1 ,
    elite > 0 ~ 1 ,
    anti_fem > 0 ~ 1 ,
    left > 0 ~ 1,
    imm == 2 ~ 1 ,
    vict == 3 ~ 1 ,
    # anti_fem > 1 ~ 1 ,
    TRUE ~ 0
  )) %>% 
  mutate(altlight = ifelse(altright == 1 & altlight == 1, 0, altlight)) %>% 
  mutate(alt_type = case_when(
    altright == 1 ~ 2,
    altlight == 1 ~ 1,
    altright == 0 & altlight == 0 ~ 0
  )) %>% 
  arrange(desc(altright), desc(altlight)) 

table(df_all$alt_type)
glimpse(df_all)
```

```{r}
df_all %>%
  group_by(platform) %>%
  summarise(na = sum(!is.na(likes)))

df_all %>%
  filter(likes > 100) %>%
  mutate(alt_type = factor(alt_type, levels = c("altright", "altlight", "none"))) %>%
  ggplot(aes(alt_type, likes, fill = alt_type)) +
  geom_boxplot()

df_all %>%
  filter(shares > 500) %>%
  mutate(alt_type = factor(alt_type, levels = c("altright", "altlight", "none"))) %>%
  ggplot(aes(alt_type, shares, fill = alt_type)) +
  geom_boxplot()

df_all %>%
  filter(comments > 100) %>%
  mutate(alt_type = factor(alt_type, levels = c("altright", "altlight", "none"))) %>%
  ggplot(aes(alt_type, comments, fill = alt_type)) +
  geom_boxplot()


df_all %>%
  filter(likes > 5) %>%
  mutate(race = as.factor(race)) %>%
  mutate(alt_type = factor(alt_type, levels = c("altright", "altlight", "none"))) %>%
  ggplot(aes(race, likes, fill = race)) +
  geom_boxplot() +
  geom_violin()

df_all %>%
  filter(likes > 10) %>%
  mutate(anti_sem = as.factor(anti_sem)) %>%
  mutate(alt_type = factor(alt_type, levels = c("altright", "altlight", "none"))) %>%
  ggplot(aes(anti_sem, likes, fill = anti_sem)) +
  geom_violin()
# table(is.na(df_all$likes))

df_all %>%
  dplyr::select(likes, race, anti_sem) %>%
  tidyr::gather("var", "value", -likes) %>%
  #filter(likes > 0) %>%
  mutate(value = as.factor(value)) %>%
  group_by(var, value) %>%
  summarise(m = median(likes, na.rm = T), s = sd(likes, na.rm = T))
  ggplot(aes(var, likes, fill = value)) +
  # geom_violin() 
  #geom_boxplot(outlier.colour = NA) +
  ylim(0, 10)
  #facet_wrap(~var)
```



## Data Preperation


```{r}
prep <- df_all %>%
  mutate(id = 1:n()) %>%
  tidytext::unnest_tokens(word, text, to_lower = F) %>% 
  left_join(tidyTX::hash_lemma_en, by = "word") %>%
  mutate(lemma = ifelse(is.na(lemma), word, lemma)) %>%
  dplyr::anti_join(tidyTX::stop_words_en, by = "word") %>%
  filter(!stringr::str_detect(lemma, "[[:digit:]]|[[:punct:]]")) %>%
  # 5. Remove words shorter than 2 character
  filter(nchar(word) > 1) %>%
  #mutate(index = 1:n()) %>%
    # 6. Split documents into text batches of length(maxlen)
  group_by(id) %>%
  summarise(
    text_word = paste(words, collapse = " "),
    text_lemma = paste(lemma, collapse = " ")) %>%
  ungroup() %>%
  bind_cols(., df_all %>% dplyr::select(-text)) %>%
  arrange(sample(1:length(id), length(id))) %>%
  #dplyr::select(id, anti_fem, text_lemma, text_word) %>%
  rename(index = id) 

target <- "alt_type"
# prep1 <- bind_rows(
#   prep %>%
#     filter(left == 1) %>%
#     slice(1:1000),
#   prep %>%
#     filter(left == 2) %>%
#     slice(1:619)
# )
#   
  
final <- prep %>%
  tidyMBO:: split_data(p = .8)
```

```{r}
corpus_description <- function(data, text){
  dat <- data %>%
    dplyr::rename_("text" = text) %>%
    dplyr::mutate(nchar = text %>% nchar())  %>%
    dplyr::mutate(ntok = tidyTX::tx_n_tokens(text))
  
  tc <- dat %>%
    dplyr::select(text) %>%
    tidytext::unnest_tokens(word, text, token = "words") %>% 
    dplyr::count(word) %>% 
    dplyr::arrange(desc(n)) 
  
  out <- list(
    char = list(
      mean = mean(dat$nchar, na.rm = T) %>% floor(),
      med = median(dat$nchar, na.rm = T) 
    ),
    token = list(
      mean = mean(dat$ntok, na.rm = T) %>% floor(),
      med = median(dat$ntok, na.rm = T),
      quant = quantile(dat$ntok),
      denc = quantile(dat$ntok, probs = seq(.1:1, by = .1)),
      n_5 = tc %>%
        filter(n > 5) %>%
        nrow(),
      n_3 = tc %>%
        filter(n > 3) %>% 
        nrow(),
      n_all = tc %>%
        nrow(),
      tokens = tc
    )
  )
  return(out)
}
explore <- corpus_description(data = final$train, text = "text_lemma")
explore$token$n_5
#prep$train %>% head()

#listLearners("regr", properties = c("factors", "se"))
#listLearnerProperties("regr")
```


```{r}
max_features <- 2500 # top most common words
batch_size <- 10
maxlen <- 15 # Cut texts after this number of words (called earlier)

tokenizer <- text_tokenizer(num_words = max_features)
fit_text_tokenizer(tokenizer, x = final$train$text_lemma)
#keras::save_text_tokenizer(tokenizer, "data/tokenizer")
#tokenizer <- keras::load_text_tokenizer("data/tokenizer")

final$train_seq <- tokenizer %>% 
  texts_to_sequences(final$train$text_lemma) %>% 
  pad_sequences(maxlen = maxlen, value = 0)

final$test_seq <- tokenizer %>% 
  texts_to_sequences(final$test$text_lemma) %>% 
  pad_sequences(maxlen = maxlen, value = 0)
```


## One Model Run

```{r}
glove_fit <- keras_model_sequential() %>%
  layer_embedding(
    input_dim = 2500, 
    output_dim = 128, 
    input_length = 15
    ) %>%
  layer_global_average_pooling_1d() %>%
  layer_dense(3, activation = "sigmoid") %>%
  compile(
    loss = "binary_crossentropy",
    optimizer = "adam",
    metrics = "accuracy"
  )

summary(glove_fit)
glove_fit
```

```{r}
glove_hist <- glove_fit %>% 
  keras::fit(
    x = final$train_seq, 
    y = tidyTX::tx_onehot(final$train$alt_type),
    batch_size = batch_size,
    epochs = 3, 
    validation_split = .2
  )
```

```{r}
preds_glove <- glove_fit %>%
  tidyTX::tx_keras_predict(final$test_seq, 0) %>% 
  as.vector()

length(preds_glove)
length(final$test$alt_type)
table(preds_glove, final$test$alt_type)
caret::confusionMatrix(preds_glove, final$test$alt_type)
```

* different DVs
    + binary
    + trich



## Hyper Params

```{r}
params <- makeParamSet(
    makeIntegerParam("max_features", lower = 1500, upper = 2500),
    makeIntegerParam("maxlen", lower = 10, upper = 20),
    makeIntegerParam("batch_size", lower = 1, upper = 20),
    makeIntegerParam("output_dim", lower = 20, upper = 100),
    makeDiscreteParam("output_fun", values = c("softmax", "relu", "sigmoid")),
    makeDiscreteParam("arch", values = c("fasttext", "lstm"))
    ### LSTM only
    #makeIntegerParam("lstm_units", lower = 60, upper = 300, requires = quote(arch == "lstm")),
    #makeNumericParam("dropout", lower = .1, upper = .5, requires = quote(arch == "lstm")),
    #makeNumericParam("recurrent_dropout", lower = .1, upper = .5, requires = quote(arch == "lstm"))
  )
```



## Run Main

```{r, eval = F}
#prep$train$text_lemma[1]
library(magrittr)
results <- tidyMBO::run_mbo(
    data = final, 
    params = params, 
    target = "alt_type", 
    text = "text_lemma",
    name = "stack_model2", 
    n_init = 2, 
    n_main = 2,
    metric = "accuracy", # experimental stage
    parallel = F # Only Unix/Mac no Windows support
  )
# 
# save(results, file = "results_left.Rdata")
```

```{r}
final_model <- perform %>%
  arrange(desc(accuracy)) %>%
  slice(2) %>%
  select(1:6) %>%
  as.list() %>%
#final_model <- list(max_features=6830, maxlen=10, batch_size=13, output_dim=197, output_fun="relu", arch="fasttext") %>%  
  tidyMBO::run_keras_steps(data = final, target = "left", text = "text_lemma", metric = "auc", reconstruct = T)
#caret::confusionMatrix(final_model$perform, prep$test$elite)
#table(final_model$perform)
table(final$test$left, final_model$perform)
```


```{r, eval = F}
perform <- results$df
perform %>%
  arrange(desc(accuracy))
```
